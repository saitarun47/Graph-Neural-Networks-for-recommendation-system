{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5971c4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User features: 4 dimensions\n",
      "Movie features: 21 dimensions\n",
      "Training GraphSAGE model...\n",
      "Epoch 010 | Train Loss: 0.6129 | Val Loss: 0.6802 | Val AUC: 0.6183\n",
      "Epoch 020 | Train Loss: 0.5729 | Val Loss: 0.5842 | Val AUC: 0.7539\n",
      "Epoch 030 | Train Loss: 0.5361 | Val Loss: 0.5476 | Val AUC: 0.8337\n",
      "Epoch 040 | Train Loss: 0.5075 | Val Loss: 0.4913 | Val AUC: 0.8574\n",
      "Epoch 050 | Train Loss: 0.4854 | Val Loss: 0.4619 | Val AUC: 0.8751\n",
      "Epoch 060 | Train Loss: 0.4680 | Val Loss: 0.4417 | Val AUC: 0.8795\n",
      "Epoch 070 | Train Loss: 0.4545 | Val Loss: 0.4402 | Val AUC: 0.8855\n",
      "Epoch 080 | Train Loss: 0.4426 | Val Loss: 0.4235 | Val AUC: 0.8894\n",
      "Epoch 090 | Train Loss: 0.4352 | Val Loss: 0.4152 | Val AUC: 0.8925\n",
      "Epoch 100 | Train Loss: 0.4304 | Val Loss: 0.4097 | Val AUC: 0.8955\n",
      "Epoch 110 | Train Loss: 0.4234 | Val Loss: 0.4043 | Val AUC: 0.8978\n",
      "Epoch 120 | Train Loss: 0.4153 | Val Loss: 0.3972 | Val AUC: 0.9007\n",
      "Epoch 130 | Train Loss: 0.4085 | Val Loss: 0.3925 | Val AUC: 0.9021\n",
      "Epoch 140 | Train Loss: 0.4063 | Val Loss: 0.3913 | Val AUC: 0.9026\n",
      "Epoch 150 | Train Loss: 0.4068 | Val Loss: 0.3912 | Val AUC: 0.9028\n",
      "Epoch 160 | Train Loss: 0.4083 | Val Loss: 0.3918 | Val AUC: 0.9038\n",
      "Epoch 170 | Train Loss: 0.3996 | Val Loss: 0.3872 | Val AUC: 0.9048\n",
      "Epoch 180 | Train Loss: 0.3947 | Val Loss: 0.3846 | Val AUC: 0.9059\n",
      "Epoch 190 | Train Loss: 0.3958 | Val Loss: 0.3842 | Val AUC: 0.9063\n",
      "Epoch 200 | Train Loss: 0.3963 | Val Loss: 0.3835 | Val AUC: 0.9069\n",
      "Epoch 210 | Train Loss: 0.3913 | Val Loss: 0.3857 | Val AUC: 0.9062\n",
      "Epoch 220 | Train Loss: 0.3910 | Val Loss: 0.3820 | Val AUC: 0.9088\n",
      "Epoch 230 | Train Loss: 0.3974 | Val Loss: 0.3828 | Val AUC: 0.9074\n",
      "Epoch 240 | Train Loss: 0.3892 | Val Loss: 0.3794 | Val AUC: 0.9089\n",
      "Epoch 250 | Train Loss: 0.3895 | Val Loss: 0.3806 | Val AUC: 0.9082\n",
      "Epoch 260 | Train Loss: 0.3862 | Val Loss: 0.3761 | Val AUC: 0.9101\n",
      "Epoch 270 | Train Loss: 0.3861 | Val Loss: 0.3789 | Val AUC: 0.9092\n",
      "Epoch 280 | Train Loss: 0.3865 | Val Loss: 0.3755 | Val AUC: 0.9105\n",
      "Epoch 290 | Train Loss: 0.3867 | Val Loss: 0.3754 | Val AUC: 0.9107\n",
      "Epoch 300 | Train Loss: 0.3827 | Val Loss: 0.3787 | Val AUC: 0.9093\n",
      "Epoch 310 | Train Loss: 0.3824 | Val Loss: 0.3766 | Val AUC: 0.9101\n",
      "Epoch 320 | Train Loss: 0.3829 | Val Loss: 0.3751 | Val AUC: 0.9110\n",
      "Epoch 330 | Train Loss: 0.3842 | Val Loss: 0.3751 | Val AUC: 0.9108\n",
      "Epoch 340 | Train Loss: 0.3805 | Val Loss: 0.3704 | Val AUC: 0.9128\n",
      "Epoch 350 | Train Loss: 0.3794 | Val Loss: 0.3754 | Val AUC: 0.9108\n",
      "Epoch 360 | Train Loss: 0.3786 | Val Loss: 0.3708 | Val AUC: 0.9129\n",
      "Epoch 370 | Train Loss: 0.3775 | Val Loss: 0.3718 | Val AUC: 0.9126\n",
      "Epoch 380 | Train Loss: 0.3784 | Val Loss: 0.3675 | Val AUC: 0.9143\n",
      "Epoch 390 | Train Loss: 0.3791 | Val Loss: 0.3695 | Val AUC: 0.9134\n",
      "Epoch 400 | Train Loss: 0.3742 | Val Loss: 0.3714 | Val AUC: 0.9127\n",
      "Epoch 410 | Train Loss: 0.3757 | Val Loss: 0.3679 | Val AUC: 0.9142\n",
      "Epoch 420 | Train Loss: 0.3764 | Val Loss: 0.3666 | Val AUC: 0.9146\n",
      "Epoch 430 | Train Loss: 0.3769 | Val Loss: 0.3689 | Val AUC: 0.9142\n",
      "Epoch 440 | Train Loss: 0.3748 | Val Loss: 0.3683 | Val AUC: 0.9140\n",
      "Epoch 450 | Train Loss: 0.3735 | Val Loss: 0.3665 | Val AUC: 0.9148\n",
      "Epoch 460 | Train Loss: 0.3740 | Val Loss: 0.3645 | Val AUC: 0.9156\n",
      "Epoch 470 | Train Loss: 0.3757 | Val Loss: 0.3657 | Val AUC: 0.9150\n",
      "Epoch 480 | Train Loss: 0.3756 | Val Loss: 0.3675 | Val AUC: 0.9144\n",
      "Epoch 490 | Train Loss: 0.3729 | Val Loss: 0.3661 | Val AUC: 0.9149\n",
      "Epoch 500 | Train Loss: 0.3728 | Val Loss: 0.3637 | Val AUC: 0.9160\n",
      "Epoch 510 | Train Loss: 0.3731 | Val Loss: 0.3645 | Val AUC: 0.9156\n",
      "Epoch 520 | Train Loss: 0.3715 | Val Loss: 0.3637 | Val AUC: 0.9161\n",
      "Epoch 530 | Train Loss: 0.3717 | Val Loss: 0.3668 | Val AUC: 0.9149\n",
      "Epoch 540 | Train Loss: 0.3717 | Val Loss: 0.3615 | Val AUC: 0.9169\n",
      "Epoch 550 | Train Loss: 0.3705 | Val Loss: 0.3637 | Val AUC: 0.9162\n",
      "Epoch 560 | Train Loss: 0.3693 | Val Loss: 0.3647 | Val AUC: 0.9156\n",
      "Epoch 570 | Train Loss: 0.3699 | Val Loss: 0.3639 | Val AUC: 0.9159\n",
      "Epoch 580 | Train Loss: 0.3694 | Val Loss: 0.3637 | Val AUC: 0.9159\n",
      "Epoch 590 | Train Loss: 0.3702 | Val Loss: 0.3636 | Val AUC: 0.9159\n",
      "Epoch 600 | Train Loss: 0.3700 | Val Loss: 0.3614 | Val AUC: 0.9168\n",
      "Epoch 610 | Train Loss: 0.3689 | Val Loss: 0.3652 | Val AUC: 0.9156\n",
      "Epoch 620 | Train Loss: 0.3684 | Val Loss: 0.3623 | Val AUC: 0.9167\n",
      "Epoch 630 | Train Loss: 0.3690 | Val Loss: 0.3620 | Val AUC: 0.9165\n",
      "Epoch 640 | Train Loss: 0.3681 | Val Loss: 0.3629 | Val AUC: 0.9163\n",
      "Epoch 650 | Train Loss: 0.3683 | Val Loss: 0.3626 | Val AUC: 0.9171\n",
      "Epoch 660 | Train Loss: 0.3679 | Val Loss: 0.3614 | Val AUC: 0.9170\n",
      "Epoch 670 | Train Loss: 0.3687 | Val Loss: 0.3622 | Val AUC: 0.9164\n",
      "Epoch 680 | Train Loss: 0.3676 | Val Loss: 0.3604 | Val AUC: 0.9172\n",
      "Epoch 690 | Train Loss: 0.3665 | Val Loss: 0.3609 | Val AUC: 0.9170\n",
      "Epoch 700 | Train Loss: 0.3663 | Val Loss: 0.3609 | Val AUC: 0.9171\n",
      "Epoch 710 | Train Loss: 0.3651 | Val Loss: 0.3593 | Val AUC: 0.9176\n",
      "Epoch 720 | Train Loss: 0.3647 | Val Loss: 0.3606 | Val AUC: 0.9170\n",
      "Epoch 730 | Train Loss: 0.3656 | Val Loss: 0.3621 | Val AUC: 0.9165\n",
      "Epoch 740 | Train Loss: 0.3663 | Val Loss: 0.3606 | Val AUC: 0.9171\n",
      "Epoch 750 | Train Loss: 0.3652 | Val Loss: 0.3616 | Val AUC: 0.9172\n",
      "Epoch 760 | Train Loss: 0.3670 | Val Loss: 0.3583 | Val AUC: 0.9181\n",
      "Epoch 770 | Train Loss: 0.3649 | Val Loss: 0.3584 | Val AUC: 0.9181\n",
      "Epoch 780 | Train Loss: 0.3655 | Val Loss: 0.3585 | Val AUC: 0.9179\n",
      "Epoch 790 | Train Loss: 0.3638 | Val Loss: 0.3593 | Val AUC: 0.9176\n",
      "Epoch 800 | Train Loss: 0.3653 | Val Loss: 0.3572 | Val AUC: 0.9185\n",
      "Epoch 810 | Train Loss: 0.3623 | Val Loss: 0.3578 | Val AUC: 0.9182\n",
      "Epoch 820 | Train Loss: 0.3648 | Val Loss: 0.3586 | Val AUC: 0.9183\n",
      "Epoch 830 | Train Loss: 0.3628 | Val Loss: 0.3602 | Val AUC: 0.9179\n",
      "Epoch 840 | Train Loss: 0.3632 | Val Loss: 0.3575 | Val AUC: 0.9187\n",
      "Epoch 850 | Train Loss: 0.3633 | Val Loss: 0.3583 | Val AUC: 0.9181\n",
      "Epoch 860 | Train Loss: 0.3628 | Val Loss: 0.3569 | Val AUC: 0.9186\n",
      "Epoch 870 | Train Loss: 0.3619 | Val Loss: 0.3570 | Val AUC: 0.9187\n",
      "Epoch 880 | Train Loss: 0.3626 | Val Loss: 0.3613 | Val AUC: 0.9175\n",
      "Epoch 890 | Train Loss: 0.3661 | Val Loss: 0.3565 | Val AUC: 0.9188\n",
      "Epoch 900 | Train Loss: 0.3618 | Val Loss: 0.3579 | Val AUC: 0.9182\n",
      "Epoch 910 | Train Loss: 0.3633 | Val Loss: 0.3575 | Val AUC: 0.9185\n",
      "Epoch 920 | Train Loss: 0.3623 | Val Loss: 0.3558 | Val AUC: 0.9191\n",
      "Epoch 930 | Train Loss: 0.3618 | Val Loss: 0.3576 | Val AUC: 0.9186\n",
      "Epoch 940 | Train Loss: 0.3627 | Val Loss: 0.3561 | Val AUC: 0.9190\n",
      "Epoch 950 | Train Loss: 0.3613 | Val Loss: 0.3562 | Val AUC: 0.9190\n",
      "Epoch 960 | Train Loss: 0.3627 | Val Loss: 0.3561 | Val AUC: 0.9189\n",
      "Epoch 970 | Train Loss: 0.3611 | Val Loss: 0.3561 | Val AUC: 0.9189\n",
      "Epoch 980 | Train Loss: 0.3606 | Val Loss: 0.3564 | Val AUC: 0.9187\n",
      "Epoch 990 | Train Loss: 0.3608 | Val Loss: 0.3558 | Val AUC: 0.9190\n",
      "\n",
      "==================================================\n",
      "GRAPHSAGE RESULTS\n",
      "==================================================\n",
      "Test Loss: 0.3527\n",
      "Test AUC: 0.9204\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.nn import SAGEConv  # Changed from GCNConv to SAGEConv\n",
    "import torch.nn as nn\n",
    "\n",
    "# ──────────────── 1. LOAD DATA (Same as before) ────────────────\n",
    "df_ratings = pd.read_csv(\"u.data\", sep=\"\\t\", header=None, \n",
    "                        names=[\"user_id\", \"item_id\", \"rating\", \"timestamp\"])\n",
    "df_users = pd.read_csv(\"u.user\", sep=\"|\", header=None, \n",
    "                      names=[\"user_id\", \"age\", \"gender\", \"occupation\", \"zip_code\"])\n",
    "\n",
    "item_cols = [\"movie_id\", \"movie_title\", \"release_date\", \"video_release_date\", \"IMDb_URL\",\n",
    "            \"unknown\", \"Action\", \"Adventure\", \"Animation\", \"Children's\", \"Comedy\",\n",
    "            \"Crime\", \"Documentary\", \"Drama\", \"Fantasy\", \"Film-Noir\", \"Horror\",\n",
    "            \"Musical\", \"Mystery\", \"Romance\", \"Sci-Fi\", \"Thriller\", \"War\", \"Western\"]\n",
    "df_items = pd.read_csv(\"u.item\", sep=\"|\", header=None, names=item_cols, encoding=\"latin-1\")\n",
    "\n",
    "# ──────────────── 2. SIMPLE FEATURE IMPROVEMENTS (Same as before) ────────────────\n",
    "user_id_map = {raw: idx for idx, raw in enumerate(df_users[\"user_id\"])}\n",
    "movie_id_map = {raw: idx for idx, raw in enumerate(df_items[\"movie_id\"])}\n",
    "\n",
    "df_ratings[\"user_id_mapped\"] = df_ratings[\"user_id\"].map(user_id_map)\n",
    "df_ratings[\"movie_id_mapped\"] = df_ratings[\"item_id\"].map(movie_id_map)\n",
    "\n",
    "num_users = len(user_id_map)\n",
    "num_movies = len(movie_id_map)\n",
    "\n",
    "def create_simple_user_features(df_users, df_ratings):\n",
    "    age_scaled = MinMaxScaler().fit_transform(df_users[[\"age\"]])\n",
    "    occ_enc = LabelEncoder().fit(df_users[\"occupation\"])\n",
    "    occ_encoded = occ_enc.transform(df_users[\"occupation\"])[:, None]\n",
    "    \n",
    "    user_stats = df_ratings.groupby('user_id').agg({\n",
    "        'rating': ['mean', 'count']\n",
    "    }).fillna(0)\n",
    "    user_stats.columns = ['avg_rating', 'num_ratings']\n",
    "    \n",
    "    stats_scaled = MinMaxScaler().fit_transform(user_stats.values)\n",
    "    features = np.hstack([age_scaled, occ_encoded, stats_scaled])\n",
    "    return torch.tensor(features, dtype=torch.float)\n",
    "\n",
    "def create_simple_movie_features(df_items, df_ratings):\n",
    "    genre_features = df_items[item_cols[5:]].values\n",
    "    \n",
    "    movie_stats = df_ratings.groupby('item_id').agg({\n",
    "        'rating': ['mean', 'count']\n",
    "    }).fillna(0)\n",
    "    movie_stats.columns = ['avg_rating', 'num_ratings']\n",
    "    \n",
    "    stats_scaled = MinMaxScaler().fit_transform(movie_stats.values)\n",
    "    features = np.hstack([genre_features, stats_scaled])\n",
    "    return torch.tensor(features, dtype=torch.float)\n",
    "\n",
    "u_feats = create_simple_user_features(df_users, df_ratings)\n",
    "m_feats = create_simple_movie_features(df_items, df_ratings)\n",
    "\n",
    "print(f\"User features: {u_feats.shape[1]} dimensions\")\n",
    "print(f\"Movie features: {m_feats.shape[1]} dimensions\")\n",
    "\n",
    "# ──────────────── 3. BUILD GRAPH (Same as before) ────────────────\n",
    "hetero = HeteroData()\n",
    "hetero[\"user\"].x = u_feats\n",
    "hetero[\"movie\"].x = m_feats\n",
    "\n",
    "edge_index = torch.tensor([df_ratings[\"user_id_mapped\"].values,\n",
    "                          df_ratings[\"movie_id_mapped\"].values], dtype=torch.long)\n",
    "hetero[\"user\", \"rates\", \"movie\"].edge_index = edge_index\n",
    "\n",
    "data = hetero.to_homogeneous(node_attrs=[\"x\"], edge_attrs=None)\n",
    "\n",
    "# ──────────────── 4. DATA SPLITTING (Same as before) ────────────────\n",
    "pos_df = df_ratings[[\"user_id_mapped\", \"movie_id_mapped\"]]\n",
    "train_val, test_df = train_test_split(pos_df, test_size=0.10, random_state=42)\n",
    "train_df, val_df = train_test_split(train_val, test_size=0.1111, random_state=42)\n",
    "\n",
    "all_pos = set(zip(pos_df.user_id_mapped, pos_df.movie_id_mapped))\n",
    "\n",
    "def sample_neg(n):\n",
    "    negs = set()\n",
    "    while len(negs) < n:\n",
    "        u = random.randrange(num_users)\n",
    "        v = random.randrange(num_movies)\n",
    "        if (u, v) not in all_pos:\n",
    "            negs.add((u, v))\n",
    "    return pd.DataFrame(list(negs), columns=[\"user_id_mapped\",\"movie_id_mapped\"])\n",
    "\n",
    "neg_train = sample_neg(len(train_df))\n",
    "neg_val = sample_neg(len(val_df))\n",
    "neg_test = sample_neg(len(test_df))\n",
    "\n",
    "def build_edges(pos, neg):\n",
    "    u_list = list(pos.user_id_mapped) + list(neg.user_id_mapped)\n",
    "    m_list = [m + num_users for m in list(pos.movie_id_mapped) + list(neg.movie_id_mapped)]\n",
    "    ei = torch.tensor([u_list, m_list], dtype=torch.long)\n",
    "    lbl = torch.tensor([1]*len(pos) + [0]*len(neg), dtype=torch.float)\n",
    "    return ei, lbl\n",
    "\n",
    "train_ei, train_lbl = build_edges(train_df, neg_train)\n",
    "val_ei, val_lbl = build_edges(val_df, neg_val)\n",
    "test_ei, test_lbl = build_edges(test_df, neg_test)\n",
    "\n",
    "# ──────────────── 5. GRAPHSAGE MODEL (MAIN CHANGE HERE) ────────────────\n",
    "class SimpleGraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, in_feats, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        # CHANGED: Using SAGEConv instead of GCNConv\n",
    "        self.conv1 = SAGEConv(in_feats, hidden_dim)\n",
    "        self.conv2 = SAGEConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = SAGEConv(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # Same batch normalization as before\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "        # Same decoder as before\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        # Layer 1\n",
    "        h1 = self.conv1(x, edge_index)\n",
    "        h1 = self.bn1(h1)\n",
    "        h1 = F.relu(h1)\n",
    "        h1 = F.dropout(h1, p=0.3, training=self.training)\n",
    "        \n",
    "        # Layer 2\n",
    "        h2 = self.conv2(h1, edge_index)\n",
    "        h2 = self.bn2(h2)\n",
    "        h2 = F.relu(h2)\n",
    "        h2 = F.dropout(h2, p=0.3, training=self.training)\n",
    "        \n",
    "        # Layer 3\n",
    "        h3 = self.conv3(h2, edge_index)\n",
    "        return h3\n",
    "\n",
    "    def decode(self, z, edge_label_index):\n",
    "        src, dst = edge_label_index\n",
    "        h = torch.cat([z[src], z[dst]], dim=1)\n",
    "        return self.decoder(h).view(-1)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_label_index):\n",
    "        z = self.encode(x, edge_index)\n",
    "        return self.decode(z, edge_label_index)\n",
    "\n",
    "# ──────────────── 6. TRAINING (Same as before) ────────────────\n",
    "def train_graphsage_model():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = SimpleGraphSAGE(data.num_features, hidden_dim=128).to(device)  # Changed model name\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    x = data.x.to(device)\n",
    "    ei = data.edge_index.to(device)\n",
    "    \n",
    "    best_val_auc = 0\n",
    "    patience_counter = 0\n",
    "    patience = 20\n",
    "    \n",
    "    print(\"Training GraphSAGE model...\")\n",
    "    \n",
    "    for epoch in range(1, 1000):\n",
    "        # Training\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x, ei, train_ei.to(device))\n",
    "        loss = loss_fn(out, train_lbl.to(device))\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        # Validation every 10 epochs\n",
    "        if epoch % 10 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_out = model(x, ei, val_ei.to(device))\n",
    "                val_loss = loss_fn(val_out, val_lbl.to(device))\n",
    "                \n",
    "                val_pred = torch.sigmoid(val_out).cpu().numpy()\n",
    "                val_auc = roc_auc_score(val_lbl.numpy(), val_pred)\n",
    "                \n",
    "                print(f\"Epoch {epoch:03d} | Train Loss: {loss:.4f} | Val Loss: {val_loss:.4f} | Val AUC: {val_auc:.4f}\")\n",
    "                \n",
    "                if val_auc > best_val_auc:\n",
    "                    best_val_auc = val_auc\n",
    "                    patience_counter = 0\n",
    "                    best_model = model.state_dict().copy()\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    \n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch}\")\n",
    "                    break\n",
    "    \n",
    "    model.load_state_dict(best_model)\n",
    "    return model\n",
    "\n",
    "# ──────────────── 7. TRAIN AND EVALUATE ────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    # Train the GraphSAGE model\n",
    "    model = train_graphsage_model()\n",
    "    \n",
    "    # Final evaluation\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    x = data.x.to(device)\n",
    "    ei = data.edge_index.to(device)\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_out = model(x, ei, test_ei.to(device))\n",
    "        test_loss = loss_fn(test_out, test_lbl.to(device))\n",
    "        test_pred = torch.sigmoid(test_out).cpu().numpy()\n",
    "        test_auc = roc_auc_score(test_lbl.numpy(), test_pred)\n",
    "        \n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(\"GRAPHSAGE RESULTS\")\n",
    "        print(f\"{'='*50}\")\n",
    "        print(f\"Test Loss: {test_loss:.4f}\")\n",
    "        print(f\"Test AUC: {test_auc:.4f}\")\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d8dc71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cuda_env)",
   "language": "python",
   "name": "cuda_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
