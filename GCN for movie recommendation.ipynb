{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b2deafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tanuj\\AppData\\Local\\Temp\\ipykernel_24500\\263163485.py:74: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  edge_index = torch.tensor([\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 — train_loss: 0.6781, val_loss: 0.5972\n",
      "Epoch 20 — train_loss: 0.6061, val_loss: 0.5854\n",
      "Epoch 30 — train_loss: 0.5796, val_loss: 0.5815\n",
      "Epoch 40 — train_loss: 0.5670, val_loss: 0.5754\n",
      "Epoch 50 — train_loss: 0.5643, val_loss: 0.5690\n",
      "Epoch 60 — train_loss: 0.5523, val_loss: 0.5734\n",
      "Epoch 70 — train_loss: 0.5541, val_loss: 0.5794\n",
      "Epoch 80 — train_loss: 0.5548, val_loss: 0.5762\n",
      "Epoch 90 — train_loss: 0.5438, val_loss: 0.5629\n",
      "Epoch 100 — train_loss: 0.5385, val_loss: 0.5644\n",
      "Epoch 110 — train_loss: 0.5386, val_loss: 0.5656\n",
      "Epoch 120 — train_loss: 0.5318, val_loss: 0.5749\n",
      "Epoch 130 — train_loss: 0.5314, val_loss: 0.5667\n",
      "Epoch 140 — train_loss: 0.5250, val_loss: 0.5939\n",
      "Epoch 150 — train_loss: 0.5247, val_loss: 0.5445\n",
      "Epoch 160 — train_loss: 0.5219, val_loss: 0.5702\n",
      "Epoch 170 — train_loss: 0.5125, val_loss: 0.5445\n",
      "Epoch 180 — train_loss: 0.5115, val_loss: 0.5548\n",
      "Epoch 190 — train_loss: 0.5039, val_loss: 0.5348\n",
      "Epoch 200 — train_loss: 0.4987, val_loss: 0.5440\n",
      "Epoch 210 — train_loss: 0.5003, val_loss: 0.5384\n",
      "Epoch 220 — train_loss: 0.5033, val_loss: 0.5536\n",
      "Epoch 230 — train_loss: 0.4950, val_loss: 0.5288\n",
      "Epoch 240 — train_loss: 0.4953, val_loss: 0.5352\n",
      "Epoch 250 — train_loss: 0.4984, val_loss: 0.5286\n",
      "Epoch 260 — train_loss: 0.4978, val_loss: 0.5348\n",
      "Epoch 270 — train_loss: 0.4926, val_loss: 0.5249\n",
      "Epoch 280 — train_loss: 0.4926, val_loss: 0.5337\n",
      "Epoch 290 — train_loss: 0.4891, val_loss: 0.5229\n",
      "Epoch 300 — train_loss: 0.4938, val_loss: 0.5265\n",
      "Epoch 310 — train_loss: 0.4906, val_loss: 0.5372\n",
      "Epoch 320 — train_loss: 0.4893, val_loss: 0.5346\n",
      "Epoch 330 — train_loss: 0.4901, val_loss: 0.5370\n",
      "Epoch 340 — train_loss: 0.4923, val_loss: 0.5354\n",
      "Epoch 350 — train_loss: 0.4874, val_loss: 0.5295\n",
      "Epoch 360 — train_loss: 0.4928, val_loss: 0.5438\n",
      "Epoch 370 — train_loss: 0.4903, val_loss: 0.5345\n",
      "Epoch 380 — train_loss: 0.4902, val_loss: 0.5314\n",
      "Epoch 390 — train_loss: 0.4885, val_loss: 0.5543\n",
      "Epoch 400 — train_loss: 0.4893, val_loss: 0.5264\n",
      "Epoch 410 — train_loss: 0.4872, val_loss: 0.5565\n",
      "Epoch 420 — train_loss: 0.4911, val_loss: 0.5258\n",
      "Epoch 430 — train_loss: 0.4874, val_loss: 0.5460\n",
      "Epoch 440 — train_loss: 0.4894, val_loss: 0.5279\n",
      "Epoch 450 — train_loss: 0.4877, val_loss: 0.5430\n",
      "Epoch 460 — train_loss: 0.4865, val_loss: 0.5359\n",
      "Epoch 470 — train_loss: 0.4866, val_loss: 0.5521\n",
      "Epoch 480 — train_loss: 0.4884, val_loss: 0.5332\n",
      "Epoch 490 — train_loss: 0.4872, val_loss: 0.5497\n",
      "Epoch 500 — train_loss: 0.4855, val_loss: 0.5306\n",
      "Epoch 510 — train_loss: 0.4854, val_loss: 0.5445\n",
      "Epoch 520 — train_loss: 0.4857, val_loss: 0.5409\n",
      "Epoch 530 — train_loss: 0.4857, val_loss: 0.5356\n",
      "Epoch 540 — train_loss: 0.4864, val_loss: 0.5458\n",
      "Epoch 550 — train_loss: 0.4834, val_loss: 0.5415\n",
      "Epoch 560 — train_loss: 0.4849, val_loss: 0.5353\n",
      "Epoch 570 — train_loss: 0.4882, val_loss: 0.5436\n",
      "Epoch 580 — train_loss: 0.4866, val_loss: 0.5608\n",
      "Epoch 590 — train_loss: 0.4828, val_loss: 0.5524\n",
      "Epoch 600 — train_loss: 0.4880, val_loss: 0.5351\n",
      "Epoch 610 — train_loss: 0.4825, val_loss: 0.5425\n",
      "Epoch 620 — train_loss: 0.4828, val_loss: 0.5389\n",
      "Epoch 630 — train_loss: 0.4818, val_loss: 0.5422\n",
      "Epoch 640 — train_loss: 0.4817, val_loss: 0.5657\n",
      "Epoch 650 — train_loss: 0.4874, val_loss: 0.5425\n",
      "Epoch 660 — train_loss: 0.4824, val_loss: 0.5457\n",
      "Epoch 670 — train_loss: 0.4835, val_loss: 0.5462\n",
      "Epoch 680 — train_loss: 0.4805, val_loss: 0.5403\n",
      "Epoch 690 — train_loss: 0.4816, val_loss: 0.5573\n",
      "Epoch 700 — train_loss: 0.4813, val_loss: 0.5495\n",
      "Epoch 710 — train_loss: 0.4793, val_loss: 0.5431\n",
      "Epoch 720 — train_loss: 0.4823, val_loss: 0.5836\n",
      "Epoch 730 — train_loss: 0.4823, val_loss: 0.5481\n",
      "Epoch 740 — train_loss: 0.4824, val_loss: 0.5321\n",
      "Epoch 750 — train_loss: 0.4820, val_loss: 0.5480\n",
      "Epoch 760 — train_loss: 0.4802, val_loss: 0.5391\n",
      "Epoch 770 — train_loss: 0.4829, val_loss: 0.5560\n",
      "Epoch 780 — train_loss: 0.4830, val_loss: 0.5367\n",
      "Epoch 790 — train_loss: 0.4806, val_loss: 0.5508\n",
      "Epoch 800 — train_loss: 0.4809, val_loss: 0.5508\n",
      "Epoch 810 — train_loss: 0.4791, val_loss: 0.5561\n",
      "Epoch 820 — train_loss: 0.4814, val_loss: 0.5468\n",
      "Epoch 830 — train_loss: 0.4782, val_loss: 0.5425\n",
      "Epoch 840 — train_loss: 0.4781, val_loss: 0.5631\n",
      "Epoch 850 — train_loss: 0.4805, val_loss: 0.5550\n",
      "Epoch 860 — train_loss: 0.4787, val_loss: 0.5539\n",
      "Epoch 870 — train_loss: 0.4785, val_loss: 0.5618\n",
      "Epoch 880 — train_loss: 0.4789, val_loss: 0.5523\n",
      "Epoch 890 — train_loss: 0.4799, val_loss: 0.5513\n",
      "Epoch 900 — train_loss: 0.4788, val_loss: 0.5494\n",
      "Epoch 910 — train_loss: 0.4782, val_loss: 0.5572\n",
      "Epoch 920 — train_loss: 0.4787, val_loss: 0.5517\n",
      "Epoch 930 — train_loss: 0.4775, val_loss: 0.5520\n",
      "Epoch 940 — train_loss: 0.4765, val_loss: 0.5489\n",
      "Epoch 950 — train_loss: 0.4773, val_loss: 0.5599\n",
      "Epoch 960 — train_loss: 0.4758, val_loss: 0.5502\n",
      "Epoch 970 — train_loss: 0.4784, val_loss: 0.5492\n",
      "Epoch 980 — train_loss: 0.4765, val_loss: 0.5459\n",
      "Epoch 990 — train_loss: 0.4794, val_loss: 0.5592\n",
      "Epoch 1000 — train_loss: 0.4780, val_loss: 0.5523\n",
      "Epoch 1010 — train_loss: 0.4796, val_loss: 0.5508\n",
      "Epoch 1020 — train_loss: 0.4791, val_loss: 0.5487\n",
      "Epoch 1030 — train_loss: 0.4769, val_loss: 0.5559\n",
      "Epoch 1040 — train_loss: 0.4747, val_loss: 0.5519\n",
      "Epoch 1050 — train_loss: 0.4756, val_loss: 0.5704\n",
      "Epoch 1060 — train_loss: 0.4752, val_loss: 0.5704\n",
      "Epoch 1070 — train_loss: 0.4833, val_loss: 0.5663\n",
      "Epoch 1080 — train_loss: 0.4786, val_loss: 0.5515\n",
      "Epoch 1090 — train_loss: 0.4786, val_loss: 0.5431\n",
      "Epoch 1100 — train_loss: 0.4763, val_loss: 0.5520\n",
      "Epoch 1110 — train_loss: 0.4757, val_loss: 0.5578\n",
      "Epoch 1120 — train_loss: 0.4776, val_loss: 0.5463\n",
      "Epoch 1130 — train_loss: 0.4732, val_loss: 0.5436\n",
      "Epoch 1140 — train_loss: 0.4755, val_loss: 0.5555\n",
      "Epoch 1150 — train_loss: 0.4736, val_loss: 0.5469\n",
      "Epoch 1160 — train_loss: 0.4735, val_loss: 0.5498\n",
      "Epoch 1170 — train_loss: 0.4764, val_loss: 0.5544\n",
      "Epoch 1180 — train_loss: 0.4763, val_loss: 0.5415\n",
      "Epoch 1190 — train_loss: 0.4744, val_loss: 0.5479\n",
      "Epoch 1200 — train_loss: 0.4730, val_loss: 0.5515\n",
      "Epoch 1210 — train_loss: 0.4755, val_loss: 0.5510\n",
      "Epoch 1220 — train_loss: 0.4772, val_loss: 0.5591\n",
      "Epoch 1230 — train_loss: 0.4748, val_loss: 0.5521\n",
      "Epoch 1240 — train_loss: 0.4732, val_loss: 0.5570\n",
      "Epoch 1250 — train_loss: 0.4743, val_loss: 0.5513\n",
      "Epoch 1260 — train_loss: 0.4743, val_loss: 0.5416\n",
      "Epoch 1270 — train_loss: 0.4768, val_loss: 0.5530\n",
      "Epoch 1280 — train_loss: 0.4729, val_loss: 0.5665\n",
      "Epoch 1290 — train_loss: 0.4716, val_loss: 0.5554\n",
      "Epoch 1300 — train_loss: 0.4758, val_loss: 0.5551\n",
      "Epoch 1310 — train_loss: 0.4731, val_loss: 0.5450\n",
      "Epoch 1320 — train_loss: 0.4718, val_loss: 0.5500\n",
      "Epoch 1330 — train_loss: 0.4740, val_loss: 0.5493\n",
      "Epoch 1340 — train_loss: 0.4773, val_loss: 0.5441\n",
      "Epoch 1350 — train_loss: 0.4736, val_loss: 0.5467\n",
      "Epoch 1360 — train_loss: 0.4724, val_loss: 0.5429\n",
      "Epoch 1370 — train_loss: 0.4712, val_loss: 0.5322\n",
      "Epoch 1380 — train_loss: 0.4728, val_loss: 0.5305\n",
      "Epoch 1390 — train_loss: 0.4824, val_loss: 0.5646\n",
      "Epoch 1400 — train_loss: 0.4758, val_loss: 0.5545\n",
      "Epoch 1410 — train_loss: 0.4726, val_loss: 0.5482\n",
      "Epoch 1420 — train_loss: 0.4716, val_loss: 0.5576\n",
      "Epoch 1430 — train_loss: 0.4714, val_loss: 0.5469\n",
      "Epoch 1440 — train_loss: 0.4734, val_loss: 0.5463\n",
      "Epoch 1450 — train_loss: 0.4730, val_loss: 0.5472\n",
      "Epoch 1460 — train_loss: 0.4705, val_loss: 0.5428\n",
      "Epoch 1470 — train_loss: 0.4713, val_loss: 0.5422\n",
      "Epoch 1480 — train_loss: 0.4689, val_loss: 0.5465\n",
      "Epoch 1490 — train_loss: 0.4716, val_loss: 0.5562\n",
      "Epoch 1500 — train_loss: 0.4730, val_loss: 0.5394\n",
      "Epoch 1510 — train_loss: 0.4719, val_loss: 0.5359\n",
      "Epoch 1520 — train_loss: 0.4699, val_loss: 0.5591\n",
      "Epoch 1530 — train_loss: 0.4721, val_loss: 0.5468\n",
      "Epoch 1540 — train_loss: 0.4713, val_loss: 0.5487\n",
      "Epoch 1550 — train_loss: 0.4678, val_loss: 0.5482\n",
      "Epoch 1560 — train_loss: 0.4717, val_loss: 0.5611\n",
      "Epoch 1570 — train_loss: 0.4707, val_loss: 0.5382\n",
      "Epoch 1580 — train_loss: 0.4717, val_loss: 0.5415\n",
      "Epoch 1590 — train_loss: 0.4720, val_loss: 0.5512\n",
      "Epoch 1600 — train_loss: 0.4710, val_loss: 0.5494\n",
      "Epoch 1610 — train_loss: 0.4715, val_loss: 0.5380\n",
      "Epoch 1620 — train_loss: 0.4701, val_loss: 0.5533\n",
      "Epoch 1630 — train_loss: 0.4696, val_loss: 0.5406\n",
      "Epoch 1640 — train_loss: 0.4727, val_loss: 0.5504\n",
      "Epoch 1650 — train_loss: 0.4704, val_loss: 0.5517\n",
      "Epoch 1660 — train_loss: 0.4713, val_loss: 0.5402\n",
      "Epoch 1670 — train_loss: 0.4697, val_loss: 0.5483\n",
      "Epoch 1680 — train_loss: 0.4719, val_loss: 0.5525\n",
      "Epoch 1690 — train_loss: 0.4709, val_loss: 0.5458\n",
      "Epoch 1700 — train_loss: 0.4705, val_loss: 0.5452\n",
      "Epoch 1710 — train_loss: 0.4686, val_loss: 0.5541\n",
      "Epoch 1720 — train_loss: 0.4678, val_loss: 0.5411\n",
      "Epoch 1730 — train_loss: 0.4674, val_loss: 0.5446\n",
      "Epoch 1740 — train_loss: 0.4669, val_loss: 0.5462\n",
      "Epoch 1750 — train_loss: 0.4673, val_loss: 0.5465\n",
      "Epoch 1760 — train_loss: 0.4692, val_loss: 0.5429\n",
      "Epoch 1770 — train_loss: 0.4706, val_loss: 0.5432\n",
      "Epoch 1780 — train_loss: 0.4684, val_loss: 0.5562\n",
      "Epoch 1790 — train_loss: 0.4728, val_loss: 0.5501\n",
      "Epoch 1800 — train_loss: 0.4695, val_loss: 0.5378\n",
      "Epoch 1810 — train_loss: 0.4697, val_loss: 0.5422\n",
      "Epoch 1820 — train_loss: 0.4687, val_loss: 0.5428\n",
      "Epoch 1830 — train_loss: 0.4654, val_loss: 0.5454\n",
      "Epoch 1840 — train_loss: 0.4677, val_loss: 0.5446\n",
      "Epoch 1850 — train_loss: 0.4667, val_loss: 0.5362\n",
      "Epoch 1860 — train_loss: 0.4706, val_loss: 0.5527\n",
      "Epoch 1870 — train_loss: 0.4696, val_loss: 0.5466\n",
      "Epoch 1880 — train_loss: 0.4704, val_loss: 0.5509\n",
      "Epoch 1890 — train_loss: 0.4681, val_loss: 0.5372\n",
      "Epoch 1900 — train_loss: 0.4717, val_loss: 0.5284\n",
      "Epoch 1910 — train_loss: 0.4701, val_loss: 0.5517\n",
      "Epoch 1920 — train_loss: 0.4660, val_loss: 0.5347\n",
      "Epoch 1930 — train_loss: 0.4682, val_loss: 0.5477\n",
      "Epoch 1940 — train_loss: 0.4676, val_loss: 0.5360\n",
      "Epoch 1950 — train_loss: 0.4654, val_loss: 0.5409\n",
      "Epoch 1960 — train_loss: 0.4658, val_loss: 0.5451\n",
      "Epoch 1970 — train_loss: 0.4688, val_loss: 0.5425\n",
      "Epoch 1980 — train_loss: 0.4646, val_loss: 0.5362\n",
      "Epoch 1990 — train_loss: 0.4671, val_loss: 0.5520\n",
      "Test loss: 0.5386\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "# ──────────────── 1. LOAD AND PREPROCESS DATA ────────────────\n",
    "\n",
    "# 1.1 Ratings\n",
    "df_ratings = pd.read_csv(\n",
    "    \"u.data\",\n",
    "    sep=\"\\t\",                \n",
    "    header=None,\n",
    "    names=[\"user_id\", \"item_id\", \"rating\", \"timestamp\"]\n",
    ")\n",
    "\n",
    "# 1.2 Users\n",
    "df_users = pd.read_csv(\n",
    "    \"u.user\",\n",
    "    sep=\"|\",\n",
    "    header=None,\n",
    "    names=[\"user_id\", \"age\", \"gender\", \"occupation\", \"zip_code\"]\n",
    ")\n",
    "\n",
    "# 1.3 Items\n",
    "item_cols = [\n",
    "    \"movie_id\", \"movie_title\", \"release_date\", \"video_release_date\", \"IMDb_URL\",\n",
    "    \"unknown\", \"Action\", \"Adventure\", \"Animation\", \"Children's\", \"Comedy\",\n",
    "    \"Crime\", \"Documentary\", \"Drama\", \"Fantasy\", \"Film-Noir\", \"Horror\",\n",
    "    \"Musical\", \"Mystery\", \"Romance\", \"Sci-Fi\", \"Thriller\", \"War\", \"Western\"\n",
    "]\n",
    "df_items = pd.read_csv(\n",
    "    \"u.item\",\n",
    "    sep=\"|\",\n",
    "    header=None,\n",
    "    names=item_cols,\n",
    "    encoding=\"latin-1\"\n",
    ")\n",
    "\n",
    "# ──────────────── 2. BUILD HETEROGENEOUS GRAPH ────────────────\n",
    "\n",
    "# 2.1 Create 0-based mappings\n",
    "user_id_map  = {raw: idx for idx, raw in enumerate(df_users[\"user_id\"])}\n",
    "movie_id_map = {raw: idx for idx, raw in enumerate(df_items[\"movie_id\"])}\n",
    "\n",
    "df_ratings[\"user_id_mapped\"]  = df_ratings[\"user_id\"].map(user_id_map)\n",
    "df_ratings[\"movie_id_mapped\"] = df_ratings[\"item_id\"].map(movie_id_map)\n",
    "\n",
    "num_users  = len(user_id_map)\n",
    "num_movies = len(movie_id_map)\n",
    "\n",
    "# 2.2 Build node features\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "\n",
    "# Users: scale age & encode occupation\n",
    "occ_enc = LabelEncoder().fit(df_users[\"occupation\"])\n",
    "age_scaled = MinMaxScaler().fit_transform(df_users[[\"age\"]])\n",
    "occ_encoded = occ_enc.transform(df_users[\"occupation\"])[:, None]\n",
    "u_feats = torch.tensor(\n",
    "    np.hstack([age_scaled, occ_encoded]), dtype=torch.float\n",
    ")\n",
    "\n",
    "# Movies: genre one‑hots\n",
    "m_feats = torch.tensor(df_items[item_cols[5:]].values, dtype=torch.float)\n",
    "\n",
    "# 2.3 Construct HeteroData\n",
    "hetero = HeteroData()\n",
    "hetero[\"user\"].x  = u_feats\n",
    "hetero[\"movie\"].x = m_feats\n",
    "\n",
    "edge_index = torch.tensor([\n",
    "    df_ratings[\"user_id_mapped\"].values,\n",
    "    df_ratings[\"movie_id_mapped\"].values\n",
    "], dtype=torch.long)\n",
    "hetero[\"user\", \"rates\", \"movie\"].edge_index = edge_index\n",
    "\n",
    "# 2.4 Convert to homogeneous graph\n",
    "data = hetero.to_homogeneous(node_attrs=[\"x\"], edge_attrs=None)\n",
    "# Now: data.x  has shape [num_users+num_movies, feat_dim]\n",
    "#      data.edge_index has shape [2, num_edges]\n",
    "\n",
    "# ──────────────── 3. MANUAL LINK‑PREDICTION SPLIT ────────────────\n",
    "\n",
    "# 3.1 Positive edges\n",
    "pos_df = df_ratings[[\"user_id_mapped\", \"movie_id_mapped\"]]\n",
    "\n",
    "# 80/10/10 split\n",
    "train_val, test_df = train_test_split(pos_df, test_size=0.10, random_state=42)\n",
    "train_df,  val_df  = train_test_split(train_val, test_size=0.1111, random_state=42)\n",
    "\n",
    "# 3.2 Sample strict negatives (exclude all real edges)\n",
    "all_pos = set(zip(pos_df.user_id_mapped, pos_df.movie_id_mapped))\n",
    "def sample_neg(n):\n",
    "    negs = set()\n",
    "    while len(negs) < n:\n",
    "        u = random.randrange(num_users)\n",
    "        v = random.randrange(num_movies)\n",
    "        if (u, v) not in all_pos:\n",
    "            negs.add((u, v))\n",
    "    return pd.DataFrame(list(negs), columns=[\"user_id_mapped\",\"movie_id_mapped\"])\n",
    "\n",
    "neg_train = sample_neg(len(train_df))\n",
    "neg_val   = sample_neg(len(val_df))\n",
    "neg_test  = sample_neg(len(test_df))\n",
    "\n",
    "# 3.3 Build edge_index & labels and shift movie IDs\n",
    "def build_edges(pos, neg):\n",
    "    u_list = list(pos.user_id_mapped)  + list(neg.user_id_mapped)\n",
    "    m_list = [m + num_users for m in list(pos.movie_id_mapped) + list(neg.movie_id_mapped)]\n",
    "    ei = torch.tensor([u_list, m_list], dtype=torch.long)\n",
    "    lbl = torch.tensor([1]*len(pos) + [0]*len(neg), dtype=torch.float)\n",
    "    return ei, lbl\n",
    "\n",
    "train_ei, train_lbl = build_edges(train_df, neg_train)\n",
    "val_ei,   val_lbl   = build_edges(val_df,   neg_val)\n",
    "test_ei,  test_lbl  = build_edges(test_df,  neg_test)\n",
    "\n",
    "# ──────────────── 4. MODEL DEFINITION ────────────────\n",
    "\n",
    "class GNNLinkPredict(torch.nn.Module):\n",
    "    def __init__(self, in_feats, hidden_dim=64, decoder_dim=64):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_feats, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.lin1  = torch.nn.Linear(2*hidden_dim, decoder_dim)\n",
    "        self.lin2  = torch.nn.Linear(decoder_dim,     1)\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        h = self.conv1(x, edge_index)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=0.5, training=self.training)\n",
    "        return self.conv2(h, edge_index)\n",
    "\n",
    "    def decode(self, z, edge_label_index):\n",
    "        src, dst = edge_label_index\n",
    "        h = torch.cat([z[src], z[dst]], dim=1)\n",
    "        h = F.relu(self.lin1(h))\n",
    "        return self.lin2(h).view(-1)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_label_index):\n",
    "        z = self.encode(x, edge_index)\n",
    "        return self.decode(z, edge_label_index)\n",
    "\n",
    "# ──────────────── 5. TRAINING LOOP SKETCH ────────────────\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model  = GNNLinkPredict(data.num_features, 64, 64).to(device)\n",
    "opt    = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "loss_fn= torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "x  = data.x.to(device)\n",
    "ei = data.edge_index.to(device)\n",
    "\n",
    "for epoch in range(1, 2000):\n",
    "    model.train()\n",
    "    opt.zero_grad()\n",
    "    out = model(x, ei, train_ei.to(device))\n",
    "    loss = loss_fn(out, train_lbl.to(device))\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_out  = model(x, ei, val_ei.to(device))\n",
    "            val_loss = loss_fn(val_out, val_lbl.to(device))\n",
    "        print(f\"Epoch {epoch:02d} — train_loss: {loss:.4f}, val_loss: {val_loss:.4f}\")\n",
    "\n",
    "# Final test evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_out  = model(x, ei, test_ei.to(device))\n",
    "    test_loss = loss_fn(test_out, test_lbl.to(device))\n",
    "print(f\"Test loss: {test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4920b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ROC-AUC: 0.8358\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_roc_auc(model, x, edge_index, edge_label_index, labels):\n",
    "    model.eval()\n",
    "    logits = model(x, edge_index, edge_label_index)\n",
    "    probs = torch.sigmoid(logits).cpu()\n",
    "    auc = roc_auc_score(labels.cpu(), probs)\n",
    "    return auc\n",
    "\n",
    "# Compute ROC-AUC on test set\n",
    "test_auc = compute_roc_auc(model, x, ei, test_ei.to(device), test_lbl.to(device))\n",
    "print(f\"Test ROC-AUC: {test_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72365ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cuda_env)",
   "language": "python",
   "name": "cuda_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
